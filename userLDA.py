import sys
sys.path.append("..")

#Load the dataset from the CSV and save it to 'data_text'
#Importç›¸é—œè³‡æº
import matplotlib.pyplot as plt
import numpy as np
import jieba.analyse
import jieba
import codecs

# see logging events
import logging
logging.disable(50)

import os
from gensim import corpora, models, similarities
import gensim

import re
jieba.case_sensitive = True 
# å¯æ§åˆ¶å°æ–¼è©å½™ä¸­çš„è‹±æ–‡éƒ¨åˆ†æ˜¯å¦ç‚ºcase sensitive, é è¨­False
from tqdm import tqdm, trange

userdict_path='..\\dcard_linebot\\LSI\\userDict.txt'
stopword_path='..\\dcard_linebot\\LSI\\stopword.txt'
# stopword_path='../dcard_linebot/LSI/stopword.txt'
#æ¨¡å‹å­˜æ”¾è·¯å¾‘
save_path='./LDA/'

#### Load data
import pandas as pd

from pytrends.request import TrendReq
from pprint import pprint

### build a user information and personal LDA    
def load_data(userID):
    data = pd.read_csv('./data/'+str(userID)+'.csv',encoding='utf-8')

    # We only need the Headlines text column from the data
    data_text = data[:][['title','href','context']]
    data_text['index'] = data_text.index

    documents = data_text
    # print(documents)
    return documents

def createFolder(directory):
    try:
        if not os.path.exists("./LDA/"+directory):
            os.makedirs("./LDA/"+directory)
    except OSError:
        print('Error: Creating directory. '+ directory)
        
#Jiebaæ–·è©åŠå»é™¤åœç”¨å­—
def preprocess(context):
    result = []
    # if(context == 'NULL'): return result
    
    remove_msg = re.sub('[(%ã€‚ï¼Œã€‹ï¼ï¼šâ€¦ï¼Ÿ/ã€â–²â€»â–¼â–²â˜…â—ã€ï½œã€‘â—:&\'-.ã€ã€ï¼!-ã€ˆã€‰â€˜â€™\nï¼ˆï¼‰ã€Œï¼›ï½ï¼†ã„œã€°ğŸ”´ğŸ™‚ğŸ¤®ğŸˆ¶ğŸ¤“ğŸ¤§ğŸ‘ğŸ˜¯ğŸ‘†ğŸŒšğŸ˜¥ğŸ˜ƒğŸ¥´ğŸŒğŸ˜œğŸ˜ğŸ˜¨ğŸ–ğŸ‘ŒğŸ˜ğŸ‘¼ğŸ‘»ï¼ğŸ‘µğŸ‘¿ğŸ“–ğŸ”†ğŸ˜®ğŸŒŸğŸ­ğŸ‘ğŸ‘ˆğŸ˜³ğŸ˜‡ğŸ˜£ğŸ˜ğŸ˜–ğŸ˜©ğŸ˜«ğŸ˜™ğŸ˜ğŸ¤—ğŸ™‚ğŸ¤¨ğŸ””ğŸ¤©ğŸ˜¦ğŸ¤®ğŸ˜‡ğŸ’¡ğŸ™‹ğŸ§ğŸ˜ğŸ’œğŸ¤•ğŸ˜°ğŸ‘¨ğŸ‰ğŸ‰ğŸ‘‹ğŸ’»ğŸš€ğŸ“¢ğŸ£ğŸš©ğŸ‘€ğŸ”¹ğŸ”¸ğŸ”ºğŸ™‡ğŸ˜ŸğŸ˜¬ğŸ¶ğŸ¤¯ğŸ¤¬ğŸ¤ªğŸ˜ğŸ‘§ğŸ’ğŸ‘ŠğŸ˜±ğŸ”¥ğŸ™‚ğŸ˜£ğŸ¤¤ğŸ¤«ğŸ˜¥ğŸ“ŒğŸ“ğŸ³ğŸ˜ŸğŸ˜¨ğŸ˜ ğŸ‘ğŸ˜¾ğŸ¤«ğŸ‘‰ğŸ‘‡ğŸ˜‘ğŸ˜†ğŸ˜‚ğŸ˜„ğŸ˜±ğŸ‘¿ğŸ‘ŒğŸ™‡ğŸ¤µğŸ˜ŸğŸ§ï½ğŸ™ğŸ‘ğŸ’ªğŸ™„ğŸ™ƒğŸ˜’ğŸ‘‚ğŸ˜­ğŸ˜¡ğŸ¤¦ğŸ˜¢ğŸ˜…ğŸ˜€ğŸ˜­ğŸŒ¸ğŸ˜ŠğŸ¤”ğŸ˜ŠğŸ˜ğŸ˜”ğŸ˜ğŸ’¥ğŸ¦ğŸ’¦ğŸ˜µğŸ˜“ğŸ˜¡ğŸ’¸ğŸ¥ºğŸ¤·ğŸ¤¦ğŸ¤£ğŸ¥³ğŸ’©ğŸ’¢ğŸ¤¢ğŸ‘©ğŸ»ğŸ‘©ğŸ˜§ğŸ”ªğŸ˜¤ğŸ’°ğŸ˜ğŸ˜šğŸ¤­ğŸ’ğŸ’ğŸ’“ğŸ¥°ğŸ’—ğŸ’˜ğŸ¤ğŸ€ğŸ”»ğŸˆğŸ”ğŸ™‚ğŸ‘†ğŸ“ğŸ‘£ğŸ—£ğŸ¤³ğŸ™‹ğŸŒ€ğŸ‘¥ğŸ™‡)(a-zA-Z))]',"",context)
#     remove_msg = re.sub('[(é¤å»³ æœå‹™ å»šå¸« ä¸»å»š èœè‰² é£Ÿæ æ¡ç”¨ åº—å®¶ æ”¤å­ å®¢äºº ç’°å¢ƒ æ»‹å‘³ å“é … åº§ä½ è£æ½¢ æ–™ç† çƒ¹èª¿ åœ˜éšŠ çª—æˆ¶)]',"",remove_msg)
    seg_list = jieba.cut(remove_msg)
    final_msg = " ".join(seg_list)
    msg_drop = final_msg.strip().replace("  ", " ")
    msg_drop = msg_drop.strip().replace("    ", " ")
    msg_drop = msg_drop.strip().replace("  ", " ")
    final_msg= msg_drop.strip().replace("  ", " ")
    context = final_msg
    
        
    #å»é™¤åœç”¨å­—
    #è¼‰å…¥åœç”¨è©å­—å…¸
    f = open(stopword_path,encoding = 'utf-8-sig')
    stoplist = f.readlines()
    f.close()

    for i in range(len(stoplist)):
        stoplist[i] = re.sub('\n',"",stoplist[i])
        
    #é–‹å§‹æ¶ˆé™¤åœç”¨å­—
    content = context.split(' ')
    del_context=[]
    for word in range(len(content)):
        for j in range(len(stoplist)):
            if content[word] == stoplist[j]:
                del_context.append(stoplist[j])

    #print("å»é™¤å–®è©:")
    #print(del_context)

    for k in range(len(del_context)):
        try:
            content.remove(del_context[k])
        except:
            print("",end="")
            #print(del_context[k]+":å·²åˆªé™¤")

    #å»é™¤ç›¸åŒå–®è©
    temp = list(set(content))
    #æ¢å¾©åŸæœ¬æ’åˆ—
    content= sorted(temp,key=content.index)
        
    #åˆªé™¤ä¸€å€‹å­—ç¨ç«‹æˆä¸€è©çš„
    del_single_word=[]
    for l in range(len(content)):
        if len(content[l])<2:
            del_single_word.append(content[l])

    for n in range(len(del_single_word)):
        try:
            content.remove(del_single_word[n])
        except:
            print("",end="")

    #å»é™¤ç›¸åŒå–®è©
    temp = list(set(content))
    #æ¢å¾©åŸæœ¬æ’åˆ—
    content = sorted(temp,key=content.index)
    # print("save")
    return content

def build_dictionary(userID,processed_docs):
    dictionary = gensim.corpora.Dictionary(processed_docs)
#     for word,index in dictionary.token2id.items(): 
#         print(word +" id:"+ str(index))
    dictionary.save(save_path + '/'+str(userID)+"/word.dict")
    return dictionary

def build_corpus(userID,processed_docs,dictionary):
    bow_corpus = [dictionary.doc2bow(document) for document in processed_docs if document != []]
    bow_corpus
    corpora.MmCorpus.serialize(save_path+'/'+str(userID)+"/bow_corpus.mm", bow_corpus)
    return bow_corpus

def build_personal_LDA(userID):
    createFolder(str(userID))
    documents = load_data(userID)
    processed_docs = documents['context'].map(preprocess)
    print(processed_docs[:10])
    dictionary = build_dictionary(userID,processed_docs)
    bow_corpus = build_corpus(userID,processed_docs,dictionary)
    
    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=8, id2word=dictionary, passes=2)

    lda_model.save(save_path+'/'+str(userID)+'/bow_lda_model.lda')
    # For each topic, we will explore the words occuring in that topic and its relative weight
    for idx, topic in lda_model.print_topics(-1):
        print("Topic: {} \nWord: {}".format(idx, topic))
        print("\n")
